**********************
[å…¥é—¨] ä¸¤æ¬¡å¾®è°ƒLLMè·å¾—VLA
**********************

å¼€æºé¡¹ç›®åœ°å€
==========
ğŸ’Œ Bç«™è§†é¢‘ `ä¸¤æ¬¡å¾®è°ƒLLMè·å¾—VLA <https://www.bilibili.com/video/BV1pa4bzMEQx/>`_

SayCan https://say-can.github.io/

RT-1 https://robotics-transformer1.github.io/

OpenVLA https://github.com/openvla/openvla

OpnePi(Pi0, Pi0.5)  https://github.com/Physical-Intelligence/openpi

Ref
===

1. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

2. Sebastian Raschka ã€ŠBuild a Large Language Model(From Scratch)ã€‹. 2025.

3. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.

4. S. Zhao. Mathematical Foundations of Reinforcement Learning. 2025

5. Russ Tedrake. MIT. ã€ŠUnderactuated Robotics Algorithms for Walking, Running, Swimming, Flying, and Manipulationã€‹, Chapter 21,Imitation Learning. 2024

6. A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint  arXiv:2212.06817, 2022.

7. D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.

8. A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn, P. Florence, C. Fu, M. G. Arenas, K. Gopalakrishnan, K. Han, K. Hausman, A. Herzog, J. Hsu, B. Ichter, A. Irpan, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, L. Lee, T.-W. E. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao, K. Reymann, M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, R. Soricut, H. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, P. Wohlhart, J. Wu, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In arXiv preprint arXiv:2307.15818, 2023.

9. PengÂ Jia. Li Auto. Nvidia GTC. VLA:A Leap Towards Physical Al inAutonomous Driving. https://www.nvidia.com/en-us/on-demand/session/gtc25-s72557/ 2025.03

10. FigureAI. Helix: A Vision-Language-Action Model for Generalist Humanoid Control, 2025.02

11. Physical Intelligence. Ï€0.5: a VLA with Open-World Generalization. 2025.04
